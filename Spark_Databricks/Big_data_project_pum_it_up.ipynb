{"cells":[{"cell_type":"markdown","source":["#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n# **Big data class - Final project**\n \n## **Predicting Pumps status - Pump It Up Data Driven competition**\n \nCan you predict which water pumps are faulty?\n\nUsing data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\nhttps://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/\n \nIn this Project:\n* *Part 1*: Load Data\n* *Part 1*: Pre-process data\n* *Part 2*: Train and tune models\n* *Part 3*: Predict model accuracy using SVM (sklearn)\n* *Part 4*: Predict model accuracy using Random Forest (MLlib)\n\nLook for below variable to control training data set size\n* DEBUG_SMALL = True (current value)- will train on 5K dataset, 20% of full training set is set for validation\n* DEBUG_SMALL = False - will train on full 80% of 50K dataset, 20% is set for validation"],"metadata":{}},{"cell_type":"code","source":["projectVersion = '1.0.0'"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["#### Code and Libraries\n- Python and Spark\n- XGBoost\n- MLib"],"metadata":{}},{"cell_type":"code","source":["# Data files for this assignment can be found at:\n#Two locations:\n#  s9rayjpv1493320587137\n#  5hbwc80e1492906464748\n#/FileStore/tables/s9rayjpv1493320587137/Test_set_values.csv\n#/FileStore/tables/s9rayjpv1493320587137/Training_set_labels.csv\n#/FileStore/tables/s9rayjpv1493320587137/Training_data_values.csv\ndisplay(dbutils.fs.ls('/FileStore/tables/5hbwc80e1492906464748'))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["**WARNING:** If *problem with import*, required in the cell below, is not installed, follow the instructions [here](https://databricks-staging-cloudfront.staging.cloud.databricks.com/public/c65da9a2fa40e45a2028cddebe45b54c/8637560089690848/4187311313936645/6977722904629137/05f3c2ecc3.html).\n\n**Pre-processing functions**"],"metadata":{}},{"cell_type":"code","source":["import sys\nimport os\nfrom test_helper import Test\nimport pandas as pd\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.cross_validation import train_test_split\n\nsklearn.__version__"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["import sys\nimport os\nfrom test_helper import Test\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import preprocessing\nimport random\n#from sklearn.model_selection import train_test_split\n\ndef load_data():\n    train_df = pd.read_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/Training_data_values.csv\")\n    train_label_df = pd.read_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/Training_set_labels.csv\")\n    test_df    = pd.read_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/Test_set_values.csv\")\n    \n    #train_df = train_df.drop([\"id\"],axis=1)\n    #test_df = test_df.drop([\"Id\"],axis=1)\n    return train_df, train_label_df, test_df\n\ndef drop_add_features(train_df,test_df,train_lbl_df):\n    '''\n    Remove columns:\n    1. recorded_by - same value for each instance\n    2. payment because same as payment_type \n    3. quantity_group same as quantity\n    4. waterpoint_type_group same as waterpoint_type\n    \n    Drop \"Id\" column from Train data and Train labels\n    \n    Add columns:\n    1. Add year, month and day based on date_recorded\n    '''\n    drop_columns=['recorded_by','payment','quantity_group','waterpoint_type_group']\n    date_columns = ['date_recorded']\n    \n    for df in [train_df, test_df]:\n        for col in date_columns:\n            df[col] = pd.to_datetime(df[col])\n            df[col + \"_day\"] = df[col].dt.dayofyear\n            df[col + \"_month\"] = df[col].dt.month\n            df[col + \"_year\"] = df[col].dt.year\n        df.drop(drop_columns+date_columns,axis=1,inplace=True)\n\n    for df in [train_df, train_lbl_df]:\n        df.drop('id',axis=1,inplace=True) \n     \ndef map_label_to_int(label):\n    if label=='functional':\n        return 0\n    elif label =='non functional':\n        return 1\n    else: # functional needs repair\n        return 2\n\ndef map_int_to_label(label):\n    if label == 0:\n        return 'functional'\n    elif label == 1:\n        return 'non functional'\n    else: # 2\n        return 'functional needs repair'\n\ndef map_float_to_label(label):\n    if label <= 0.5:\n        return 'functional'\n    elif label > 0.5 and label < 1.5:\n        return 'non functional'\n    else: # 2\n        return 'functional needs repair'\n    \ndef pre_process_data(train_df,test_df,train_lbl_df):\n    ''' \n        Encode all categorical columns (defined as 'object' in data-frame\n    '''     \n    \n    missing_columns=['public_meeting','permit','waterpoint_type']\n    for f in missing_columns:\n        train_df[f].fillna('Missing', inplace=True)\n        test_df[f].fillna('Missing', inplace=True)\n    \n    print(\"Before Categorical pre-processing - number of features\",len(train_df.columns))\n    categorical=['permit','public_meeting','source_class','quantity','management_group','quality_group','waterpoint_type','source_type','payment_type',\\\n                 'extraction_type_class','water_quality','basin','source']\n    train_df = pd.get_dummies(train_df,prefix_sep='_',columns=categorical)\n    test_df = pd.get_dummies(test_df,prefix_sep='_',columns=categorical)\n    print(\"After Categorical pre-processing - number of features\",len(train_df.columns))\n    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object':\n            #print(\"processing Column:..\",f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(np.unique(list(train_df[f].values) + list(test_df[f].values)))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f]       = lbl.transform(list(test_df[f].values))\n    \n    # fill NaN values\n    for f in train_df.columns:\n        if train_df[f].dtype in ['float64','int64']:\n            train_df[f].fillna(train_df[f].mean(), inplace=True)\n            test_df[f].fillna(test_df[f].mean(), inplace=True)\n    \n    train_lbl_df['status_group'] = train_lbl_df['status_group'].apply(map_label_to_int)\n        \n    return train_df,test_df, train_lbl_df\ntrain_df, train_lbl_df, test_df = load_data()\ndrop_add_features(train_df,test_df,train_lbl_df)\ntrain_df,test_df,train_lbl_df = pre_process_data(train_df, test_df, train_lbl_df)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### **SVM model**\nUse sklearn library SVM model to perform multi-class classification. For demo perposes the flag is set to run on small reaining set\n* DEBUG_SMALL = True - if set to True only 5K samples will be used for training\n* FINAL_RUN = False  - if set to False will not train model on full data set"],"metadata":{}},{"cell_type":"markdown","source":["#### *Helper Functions*"],"metadata":{}},{"cell_type":"code","source":["from sklearn import metrics\nimport time\ndef train_svc(X_train, Y_train):\n    clf = svm.SVC(decision_function_shape='ovo')\n    clf.fit(X_train, Y_train['status_group']) \n#     SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n#         decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n#         max_iter=-1, probability=False, random_state=None, shrinking=True,\n#         tol=0.001, verbose=False)\n    \n    return clf\n\ndef predict_validation_result(model,X_validate,Y_validate):\n    labels = model.predict(X_validate)\n    print(metrics.confusion_matrix(Y_validate,labels))\n\n    return metrics.accuracy_score(Y_validate,labels),labels\n\ndef scale_data(train_df, test_df):\n    '''\n    It is important to scale data for the SVM algorithm\n    Use sklearn scaler to scale validation/testing data based on training data\n    '''\n    \n    scaler = preprocessing.StandardScaler().fit(train_df)\n    train_df = scaler.transform(train_df)\n    test_df = scaler.transform(test_df)\n    \n    return train_df,test_df\n\ndef create_submission(model,test_df, test_df_ix):\n    '''\n    Run prediction using model and test_df and create submission file\n    \n    '''\n    predictions = model.predict(test_df)\n    predictions_label=[]\n    for pred in predictions:\n        predictions_label.append(map_int_to_label(pred))\n    \n    submission = pd.DataFrame(data=predictions_label,  # values\n                              index=test_df_ix,  # 1st column as index\n                              columns=[\"status_group\"])  # 1st row as the column names\n    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n    submission.to_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/submission_svm_\"+timestr+\".csv\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["#### **SVM - main run**"],"metadata":{}},{"cell_type":"code","source":["from sklearn import svm\n\nDEBUG_SMALL = True\nFINAL_RUN = False\ntrain_df, train_lbl_df, test_df = load_data()\nprint('Data is loaded')\ndrop_add_features(train_df,test_df,train_lbl_df)\ntrain_df,test_df,train_lbl_df = pre_process_data(train_df, test_df, train_lbl_df)\ntest_df_ix=test_df['id']\ntest_df.drop('id',axis=1,inplace=True)\ntrain_df,test_df = scale_data(train_df,test_df)\nrandom.seed(1234)\n\n'''\nX_train - data frame to be used for training\nY_train - label data corresponding to X_train\n'''\n\nX_train, X_validate, Y_train, Y_validate = train_test_split(train_df, train_lbl_df, test_size=0.20,random_state = 2015)\n\nif FINAL_RUN == False:\n    if DEBUG_SMALL:\n        print('Running training on small sample')\n        clf = train_svc(X_train[0:5000], Y_train[0:5000])\n    else:\n        print('Running training on full sample data')\n        clf = train_svc(X_train, Y_train)\n    accuracy, predictions = predict_validation_result(clf,X_validate,Y_validate)\n    print(\"Accuracy on validation set\", accuracy)\n    #create_submission(clf, test_df,test_df_ix )\nelse:\n    '''\n    In case submission, run model on full data set\n    '''\n    print('Running training on full data')\n    clf = train_svc(train_df, train_lbl_df)\n    #create_submission(clf, test_df,test_df_ix)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["#### **Random Forest model**\nUse MLlib Spark library and pipeline to perform multi-class classification. For demo perposes the flag is set to run on small reaining set\n* DEBUG_SMALL = True - if set to True only 5K samples will be used for training\n* FINAL_RUN = False  - if set to False will not train model on full data set"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nDEBUG_SMALL = True\nFINAL_RUN = False\ntrain_df, train_lbl_df, test_df = load_data()\nprint('Data is loaded')\ndrop_add_features(train_df,test_df,train_lbl_df)\ntrain_df,test_df,train_lbl_df = pre_process_data(train_df, test_df, train_lbl_df)\n#print(train_lbl_df)\ntrain_columns = list(train_df.columns)\n#print(train_columns)\ntrain_df['status_group'] = train_lbl_df['status_group']\n#print(train_df.head())\ntest_df_ix=test_df['id']\ntest_df.drop('id',axis=1,inplace=True)\n\n''' Create Spark Data Frame and add features/labels for the MLlib\n'''\nif DEBUG_SMALL:\n  print(\"Running training on small data-set\")\n  traindf = sqlContext.createDataFrame(train_df[0:5000])\nelse:\n  print(\"Running training on 80% data-set\")\n  traindf = sqlContext.createDataFrame(train_df)\n\n# Below transformations are done in order to brind data-frame to the format MLlib is requiring.\n# MLlib requires data-frame with two columns: labels and features. While features column is collection of all features\n#\nlabelIndexer = StringIndexer(inputCol=\"status_group\", outputCol=\"indexedLabel\").fit(traindf)\nassembler = VectorAssembler(inputCols=train_columns, outputCol=\"features\")\ntraindf = assembler.transform(traindf)\nfeatureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=3).fit(traindf)\n\n(trainingData, testData) = traindf.randomSplit([0.8, 0.2])\n\n\n#\n#Best params from sklearn {'n_estimators': 120, 'random_state': 1, 'min_samples_split': 5, 'max_features': 50, 'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1}\n#\n# MLlib RandomForestClassifier input (from documentation)\n# class pyspark.ml.classification.RandomForestClassifier(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", maxDepth=5, \n#maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\", numTrees=20, featureSubsetStrategy=\"auto\", seed=None, subsamplingRate=1.0)\n\n#########################################################################\n# Below code implementation is adopted from Spark MLlib main guide\n# https://spark.apache.org/docs/2.0.2/ml-classification-regression.html\n##########################################################################\n#Train a RandomForest model.\n#\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxDepth=19,numTrees=100, seed=1)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"status_group\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Accuracy = %g\" % (accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n"],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"Big_data_project_pum_it_up","notebookId":3827989904050330},"nbformat":4,"nbformat_minor":0}
