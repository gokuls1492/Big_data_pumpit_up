<!DOCTYPE html>
<html>
<head>
  <meta name="databricks-html-version" content="1">
<title>Big_data_project_pum_it_up - Databricks</title>

<meta charset="utf-8">
<meta name="google" content="notranslate">
<meta name="robots" content="nofollow">
<meta http-equiv="Content-Language" content="en">
<meta http-equiv="Content-Type" content="text/html; charset=UTF8">
<link rel="stylesheet"
  href="https://fonts.googleapis.com/css?family=Source+Code+Pro:400,700">

<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/css/bootstrap.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/lib/jquery-ui-bundle/jquery-ui.min.css">
<link rel="stylesheet" type="text/css" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/main.css">
<link rel="stylesheet" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/css/print.css" media="print">
<link rel="icon" type="image/png" href="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/img/favicon.ico"/>
<script>window.settings = {"enableSshKeyUI":false,"defaultInteractivePricePerDBU":0.4,"enableOnDemandClusterType":true,"enableAutoCompleteAsYouType":[],"devTierName":"Community Edition","enableJobsPrefetching":true,"workspaceFeaturedLinks":[{"linkURI":"https://docs.databricks.com/index.html","displayName":"Documentation","icon":"question"},{"linkURI":"https://docs.databricks.com/release-notes/product/index.html","displayName":"Release Notes","icon":"code"},{"linkURI":"https://docs.databricks.com/spark/latest/training/index.html","displayName":"Training & Tutorials","icon":"graduation-cap"}],"enableClearStateFeature":false,"dbcForumURL":"http://forums.databricks.com/","enableProtoClusterInfoDeltaPublisher":true,"maxCustomTags":45,"enableInstanceProfilesUIInJobs":true,"nodeInfo":{"node_types":[{"support_ssh":false,"spark_heap_memory":4800,"instance_type_id":"r3.2xlarge","spark_core_oversubscription_factor":8.0,"node_type_id":"dev-tier-node","description":"Community Optimized","support_cluster_tags":false,"container_memory_mb":6000,"node_instance_type":{"instance_type_id":"r3.2xlarge","provider":"AWS","compute_units":26.0,"number_of_ips":15,"local_disks":1,"reserved_compute_units":3.64,"memory_mb":62464,"num_cores":8,"reserved_memory_mb":4800},"memory_mb":6144,"is_hidden":false,"category":"Community Edition","num_cores":0.88,"support_ebs_volumes":false,"is_deprecated":false}],"default_node_type_id":"dev-tier-node"},"enableClusterAcls":true,"notebookRevisionVisibilityHorizon":999999,"enableTableHandler":true,"maxEbsVolumesPerInstance":10,"isAdmin":true,"deltaProcessingBatchSize":1000,"enableLargeResultDownload":true,"zoneInfos":[{"id":"us-west-2c","isDefault":true},{"id":"us-west-2b","isDefault":false},{"id":"us-west-2a","isDefault":false}],"enableCustomSpotPricingUIByTier":false,"enableEBSVolumesUIForJobs":true,"enablePublishNotebooks":true,"enableMaxConcurrentRuns":true,"enableJobAclsConfig":false,"enableFullTextSearch":false,"enableElasticSparkUI":false,"enableNewClustersCreate":true,"clusters":true,"allowRunOnPendingClusters":true,"fileStoreBase":"FileStore","enableSshKeyUIInJobs":true,"enableDetachAndAttachSubMenu":false,"configurableSparkOptionsSpec":[{"keyPattern":"spark\\.kryo(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.kryo.*","valuePatternDisplay":"*","description":"Configuration options for Kryo serialization"},{"keyPattern":"spark\\.io\\.compression\\.codec","valuePattern":"(lzf|snappy|org\\.apache\\.spark\\.io\\.LZFCompressionCodec|org\\.apache\\.spark\\.io\\.SnappyCompressionCodec)","keyPatternDisplay":"spark.io.compression.codec","valuePatternDisplay":"snappy|lzf","description":"The codec used to compress internal data such as RDD partitions, broadcast variables and shuffle outputs."},{"keyPattern":"spark\\.serializer","valuePattern":"(org\\.apache\\.spark\\.serializer\\.JavaSerializer|org\\.apache\\.spark\\.serializer\\.KryoSerializer)","keyPatternDisplay":"spark.serializer","valuePatternDisplay":"org.apache.spark.serializer.JavaSerializer|org.apache.spark.serializer.KryoSerializer","description":"Class to use for serializing objects that will be sent over the network or need to be cached in serialized form."},{"keyPattern":"spark\\.rdd\\.compress","valuePattern":"(true|false)","keyPatternDisplay":"spark.rdd.compress","valuePatternDisplay":"true|false","description":"Whether to compress serialized RDD partitions (e.g. for StorageLevel.MEMORY_ONLY_SER). Can save substantial space at the cost of some extra CPU time."},{"keyPattern":"spark\\.speculation","valuePattern":"(true|false)","keyPatternDisplay":"spark.speculation","valuePatternDisplay":"true|false","description":"Whether to use speculation (recommended off for streaming)"},{"keyPattern":"spark\\.es(\\.[^\\.]+)+","valuePattern":".*","keyPatternDisplay":"spark.es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"es(\\.([^\\.]+))+","valuePattern":".*","keyPatternDisplay":"es.*","valuePatternDisplay":"*","description":"Configuration options for ElasticSearch"},{"keyPattern":"spark\\.(storage|shuffle)\\.memoryFraction","valuePattern":"0?\\.0*([1-9])([0-9])*","keyPatternDisplay":"spark.(storage|shuffle).memoryFraction","valuePatternDisplay":"(0.0,1.0)","description":"Fraction of Java heap to use for Spark's shuffle or storage"},{"keyPattern":"spark\\.streaming\\.backpressure\\.enabled","valuePattern":"(true|false)","keyPatternDisplay":"spark.streaming.backpressure.enabled","valuePatternDisplay":"true|false","description":"Enables or disables Spark Streaming's internal backpressure mechanism (since 1.5). This enables the Spark Streaming to control the receiving rate based on the current batch scheduling delays and processing times so that the system receives only as fast as the system can process. Internally, this dynamically sets the maximum receiving rate of receivers. This rate is upper bounded by the values `spark.streaming.receiver.maxRate` and `spark.streaming.kafka.maxRatePerPartition` if they are set."},{"keyPattern":"spark\\.streaming\\.receiver\\.maxRate","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.receiver.maxRate","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which each receiver will receive data. Effectively, each stream will consume at most this number of records per second. Setting this configuration to 0 or a negative number will put no limit on the rate. See the deployment guide in the Spark Streaming programing guide for mode details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRatePerPartition","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRatePerPartition","valuePatternDisplay":"numeric","description":"Maximum rate (number of records per second) at which data will be read from each Kafka partition when using the Kafka direct stream API introduced in Spark 1.3. See the Kafka Integration guide for more details."},{"keyPattern":"spark\\.streaming\\.kafka\\.maxRetries","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.kafka.maxRetries","valuePatternDisplay":"numeric","description":"Maximum number of consecutive retries the driver will make in order to find the latest offsets on the leader of each partition (a default value of 1 means that the driver will make a maximum of 2 attempts). Only applies to the Kafka direct stream API introduced in Spark 1.3."},{"keyPattern":"spark\\.streaming\\.ui\\.retainedBatches","valuePattern":"^([0-9]{1,})$","keyPatternDisplay":"spark.streaming.ui.retainedBatches","valuePatternDisplay":"numeric","description":"How many batches the Spark Streaming UI and status APIs remember before garbage collecting."}],"enableReactNotebookComments":true,"enableAdminPasswordReset":false,"enableResetPassword":true,"maxClusterTagValueLength":255,"enableJobsSparkUpgrade":true,"enableNotebookCommandNumbers":true,"sparkVersions":[{"key":"1.6.3-db2-hadoop2-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-aba860a0ffce4f3471fb14aefdcb1d768ac66a53a5ad884c48745ef98aeb9d67","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.x-ubuntu15.10","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.4.x-ubuntu15.10","displayName":"Spark 1.4.1 (Hadoop 1)","packageLabel":"spark-image-f710650fb8aaade8e4e812368ea87c45cd8cd0b5e6894ca6c94f3354e8daa6dc","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.2.x-scala2.11","displayName":"Spark Master Branch (Experimental, Scala 2.11)","packageLabel":"spark-image-b2166ecda59b5d648306cc8a0923aa3a3bf4f25ea0d47bb42449010086f1f910","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.11","displayName":"Spark 2.1.0-db2 (Scala 2.11)","packageLabel":"spark-image-267c4490a3ab8a39acdbbd9f1d36f6decdecebf013e30dd677faff50f1d9cf8b","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.10","displayName":"Spark 2.0.0 (Scala 2.10)","packageLabel":"spark-image-073c1b52ace74f251fae2680624a0d8d184a8b57096d1c21c5ce56c29be6a37a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.10","displayName":"Spark 2.0.2-db3 (Scala 2.10)","packageLabel":"spark-image-584091dedb690de20e8cf22d9e02fdcce1281edda99eedb441a418d50e28088f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.11","displayName":"Spark 2.1.0-db1 (Scala 2.11)","packageLabel":"spark-image-e8ad5b72cf0f899dcf2b4720c1f572ab0e87a311d6113b943b4e1d4a7edb77eb","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.1-db4-scala2.11","displayName":"Spark 2.1.1-db4 (Scala 2.11)","packageLabel":"spark-image-f6e9552fae99c2dd7d4cb60afb006f23c6a44c9a9ca6951d91b9d920f2f93e66","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db2-scala2.10","displayName":"Spark 2.1.0-db2 (Scala 2.10)","packageLabel":"spark-image-a2ca4f6b58c95f78dca91b1340305ab3fe32673bd894da2fa8e1dc8a9f8d0478","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop1","displayName":"Spark 1.6.x (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.2-db4-scala2.11","displayName":"Spark 2.0.2-db4 (Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.1-ubuntu15.10-hadoop1","displayName":"Spark 1.6.1 (Hadoop 1)","packageLabel":"spark-image-21d1cac181b7b8856dd1b4214a3a734f95b5289089349db9d9c926cb87d843db","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-gpu-scala2.11","displayName":"Spark 2.0 (Auto-updating, GPU, Scala 2.11 experimental)","packageLabel":"spark-image-968b89f1d0ec32e1ee4dacd04838cae25ef44370a441224177a37980d539d83a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop1","displayName":"Spark 1.6.2 (Hadoop 1)","packageLabel":"spark-image-8cea23fb9094e174bf5815d79009f4a8e383eb86cf2909cf6c6434ed8da2a16a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.3-db1-hadoop2-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 2, Scala 2.10)","packageLabel":"spark-image-eaa8d9b990015a14e032fb2e2e15be0b8d5af9627cd01d855df728b67969d5d9","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db2-hadoop1-scala2.10","displayName":"Spark 1.6.3-db2 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-14112ea0645bea94333a571a150819ce85573cf5541167d905b7e6588645cf3b","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.2-ubuntu15.10-hadoop2","displayName":"Spark 1.6.2 (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.1-ubuntu15.10-hadoop2","displayName":"Spark 1.6.1 (Hadoop 2)","packageLabel":"spark-image-4cafdf8bc6cba8edad12f441e3b3f0a8ea27da35c896bc8290e16b41fd15496a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db2-scala2.10","displayName":"Spark 2.0.2-db2 (Scala 2.10)","packageLabel":"spark-image-36d48f22cca7a907538e07df71847dd22aaf84a852c2eeea2dcefe24c681602f","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10-scala2.11","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.11, deprecated)","packageLabel":"spark-image-8e1c50d626a52eac5a6c8129e09ae206ba9890f4523775f77af4ad6d99a64c44","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-scala2.10","displayName":"Spark 2.0 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.1-db4-scala2.10","displayName":"Spark 2.1.1-db4 (Scala 2.10)","packageLabel":"spark-image-86c0873023816ae29d99f81118dc825d6380a9968a9f7a3d0a055f5c3a8964a0","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.11","displayName":"Spark 2.0.2-db1 (Scala 2.11)","packageLabel":"spark-image-c2d623f03dd44097493c01aa54a941fc31978ebe6d759b36c75b716b2ff6ab9c","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db4-scala2.10","displayName":"Spark 2.0.2-db4 (Scala 2.10)","packageLabel":"spark-image-859e88079f97f58d50e25163b39a1943d1eeac0b6939c5a65faba986477e311a","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.5.x-ubuntu15.10","displayName":"Spark 1.5.2 (Hadoop 1)","packageLabel":"spark-image-c9d2a8abf41f157a4acc6d52bc721090346f6fea2de356f3a66e388f54481698","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.x-scala2.11","displayName":"Spark 2.0 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-7dbc7583e8271765b8a1508cb9e832768e35489bbde2c4c790bc6766aee2fd7f","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-86c0873023816ae29d99f81118dc825d6380a9968a9f7a3d0a055f5c3a8964a0","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db3-scala2.10","displayName":"Spark 2.1.0-db3 (Scala 2.10)","packageLabel":"spark-image-25a17d070af155f10c4232dcc6248e36a2eb48c24f8d4fc00f34041b86bd1626","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db2-scala2.11","displayName":"Spark 2.0.2-db2 (Scala 2.11)","packageLabel":"spark-image-4fa852ba378e97815083b96c9cada7b962a513ec23554a5fc849f7f1dd8c065a","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.3.x-ubuntu15.10","displayName":"Spark 1.3.0 (Hadoop 1)","packageLabel":"spark-image-40d2842670bc3dc178b14042501847d76171437ccf70613fa397a7a24c48b912","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.1-db1-scala2.11","displayName":"Spark 2.0.1-db1 (Scala 2.11)","packageLabel":"spark-image-10ab19f634bbfdb860446c326a9f76dc25bfa87de6403b980566279142a289ea","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.2-db3-scala2.11","displayName":"Spark 2.0.2-db3 (Scala 2.11)","packageLabel":"spark-image-7fd7aaa89d55692e429115ae7eac3b1a1dc4de705d50510995f34306b39c2397","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"1.6.3-db1-hadoop1-scala2.10","displayName":"Spark 1.6.3-db1 (Hadoop 1, Scala 2.10)","packageLabel":"spark-image-d50af1032799546b8ccbeeb76889a20c819ebc2a0e68ea20920cb30d3895d3ae","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.0.2-db1-scala2.10","displayName":"Spark 2.0.2-db1 (Scala 2.10)","packageLabel":"spark-image-654bdd6e9bad70079491987d853b4b7abf3b736fff099701501acaabe0e75c41","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.x-ubuntu15.10","displayName":"Spark 2.0 (Ubuntu 15.10, Scala 2.10, deprecated)","packageLabel":"spark-image-a659f3909d51b38d297b20532fc807ecf708cfb7440ce9b090c406ab0c1e4b7e","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.0.1-db1-scala2.10","displayName":"Spark 2.0.1-db1 (Scala 2.10)","packageLabel":"spark-image-5a13c2db3091986a4e7363006cc185c5b1108c7761ef5d0218506cf2e6643840","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.x-scala2.11","displayName":"Spark 2.1 (Auto-updating, Scala 2.11)","packageLabel":"spark-image-f6e9552fae99c2dd7d4cb60afb006f23c6a44c9a9ca6951d91b9d920f2f93e66","upgradable":true,"deprecated":false,"customerVisible":true},{"key":"2.1.0-db1-scala2.10","displayName":"Spark 2.1.0-db1 (Scala 2.10)","packageLabel":"spark-image-f0ab82a5deb7908e0d159e9af066ba05fb56e1edb35bdad41b7ad2fd62a9b546","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.0-ubuntu15.10","displayName":"Spark 1.6.0 (Hadoop 1)","packageLabel":"spark-image-10ef758029b8c7e19cd7f4fb52fff9180d75db92ca071bd94c47f3c1171a7cb5","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"1.6.x-ubuntu15.10-hadoop2","displayName":"Spark 1.6.x (Hadoop 2)","packageLabel":"spark-image-161245e66d887cd775e23286a54bab0b146143e1289f25bd1732beac454a1561","upgradable":true,"deprecated":false,"customerVisible":false},{"key":"2.0.0-ubuntu15.10-scala2.11","displayName":"Spark 2.0.0 (Scala 2.11)","packageLabel":"spark-image-b4ec141e751f201399f8358a82efee202560f7ed05e1a04a2ae8778f6324b909","upgradable":true,"deprecated":true,"customerVisible":false},{"key":"2.1.0-db3-scala2.11","displayName":"Spark 2.1.0-db3 (Scala 2.11)","packageLabel":"spark-image-ccbc6b73f158e2001fc1fb8c827bfdde425d8bd6d65cb7b3269784c28bb72c16","upgradable":true,"deprecated":false,"customerVisible":true}],"enableRestrictedClusterCreation":true,"enableFeedback":true,"enableClusterAutoScaling":false,"enableUserVisibleDefaultTags":true,"defaultNumWorkers":0,"serverContinuationTimeoutMillis":10000,"autoTerminateClustersByDefault":false,"driverStderrFilePrefix":"stderr","enableNotebookRefresh":false,"accountsOwnerUrl":"https://accounts.cloud.databricks.com/registration.html#login","driverStdoutFilePrefix":"stdout","defaultNodeTypeToPricingUnitsMap":{"r3.2xlarge":2,"class-node":1,"m4.2xlarge":0.5,"r4.xlarge":1,"m4.4xlarge":0.5,"r4.16xlarge":8,"p2.8xlarge":16,"m4.10xlarge":0.5,"r3.8xlarge":8,"r4.4xlarge":4,"dev-tier-node":1,"c3.8xlarge":4,"r3.4xlarge":4,"i2.4xlarge":6,"m4.xlarge":0.5,"r4.8xlarge":8,"r4.large":0.5,"development-node":1,"i2.2xlarge":3,"g2.8xlarge":6,"memory-optimized":1,"m4.large":0.5,"p2.16xlarge":24,"c3.2xlarge":1,"c4.2xlarge":1,"i2.xlarge":1.5,"compute-optimized":1,"c4.4xlarge":2,"c3.4xlarge":2,"g2.2xlarge":1.5,"p2.xlarge":2,"m4.16xlarge":0.5,"c4.8xlarge":4,"r3.xlarge":1,"r4.2xlarge":2,"i2.8xlarge":12},"enableSparkDocsSearch":true,"sparkHistoryServerEnabled":true,"enableEBSVolumesUI":false,"sanitizeMarkdownHtml":true,"enableIPythonImportExport":true,"enableClusterTagsUIForJobs":true,"enableClusterTagsUI":false,"enableNotebookHistoryDiffing":true,"branch":"2.44","accountsLimit":3,"enableSparkEnvironmentVariables":true,"enableX509Authentication":false,"enableStructuredStreamingNbOptimizations":false,"enableNotebookGitBranching":true,"local":false,"enableClusterAutoScalingForJobs":false,"enableStrongPassword":false,"displayDefaultContainerMemoryGB":6,"enableNotebookCommandMode":true,"disableS3TableImport":false,"deploymentMode":"production","useSpotForWorkers":true,"enableUserInviteWorkflow":true,"enableStaticNotebooks":true,"perClusterAutoTerminationEnabled":false,"enableCssTransitions":true,"defaultAutoTerminationInactivityMin":180,"minClusterTagKeyLength":1,"showHomepageFeaturedLinks":true,"pricingURL":"https://databricks.com/product/pricing","enableClusterAclsConfig":false,"useTempS3UrlForTableUpload":false,"notifyLastLogin":false,"enableSshKeyUIByTier":false,"defaultAutomatedPricePerDBU":0.2,"enableNotebookGitVersioning":true,"files":"files/","feedbackEmail":"feedback@databricks.com","enableDriverLogsUI":true,"enableWorkspaceAclsConfig":false,"dropzoneMaxFileSize":2047,"enableNewClustersList":false,"enableNewDashboardViews":true,"driverLog4jFilePrefix":"log4j","enableSingleSignOn":true,"enableMavenLibraries":true,"displayRowLimit":1000,"deltaProcessingAsyncEnabled":true,"enableSparkEnvironmentVariablesUI":false,"defaultSparkVersion":{"key":"2.1.x-scala2.10","displayName":"Spark 2.1 (Auto-updating, Scala 2.10)","packageLabel":"spark-image-86c0873023816ae29d99f81118dc825d6380a9968a9f7a3d0a055f5c3a8964a0","upgradable":true,"deprecated":false,"customerVisible":true},"enableCustomSpotPricing":false,"enableMountAclsConfig":false,"useDevTierHomePage":true,"enableClusterClone":true,"enableNotebookLineNumbers":true,"enablePublishHub":false,"minAutoTerminationInactivityMin":10,"notebookHubUrl":"http://hub.dev.databricks.com/","showSqlEndpoints":false,"enableClusterAclsByTier":false,"databricksDocsBaseUrl":"https://docs.databricks.com/","cloud":"AWS","disallowAddingAdmins":true,"enableSparkConfUI":true,"featureTier":"DEVELOPER_BASIC_TIER","mavenCentralSearchEndpoint":"http://search.maven.org/solrsearch/select","enableOrgSwitcherUI":true,"clustersLimit":1,"enableJdbcImport":true,"logfiles":"logfiles/","enableWebappSharding":true,"enableClusterDeltaUpdates":true,"enableSingleSignOnLogin":false,"ebsVolumeSizeLimitGB":{"GENERAL_PURPOSE_SSD":[100,4096],"THROUGHPUT_OPTIMIZED_HDD":[500,4096]},"enableMountAcls":false,"requireEmailUserName":true,"dbcFeedbackURL":"mailto:feedback@databricks.com","enableMountAclService":true,"enableWorkspaceAcls":false,"maxClusterTagKeyLength":127,"gitHash":"16bec05fa08fe1dd3350ef898eaf4457d7c34e6a","showWorkspaceFeaturedLinks":true,"signupUrl":"https://databricks.com/try-databricks","allowFeedbackForumAccess":true,"enableImportFromUrl":true,"enableMiniClusters":true,"enableNewTableUI":true,"enableDebugUI":false,"enableStreamingMetricsDashboard":true,"allowNonAdminUsers":true,"enableSingleSignOnByTier":false,"enableJobsRetryOnTimeout":true,"useStandardTierUpgradeTooltips":true,"staticNotebookResourceUrl":"https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/","enableSpotClusterType":true,"enableSparkPackages":true,"dynamicSparkVersions":true,"enableClusterTagsUIByTier":false,"enableNotebookHistoryUI":true,"enableClusterLoggingUI":true,"maxAutoTerminationInactivityMin":10000,"enableDatabaseDropdownInTableUI":false,"showDebugCounters":false,"enableInstanceProfilesUI":false,"enableFolderHtmlExport":true,"homepageFeaturedLinks":[{"linkURI":"https://docs.databricks.com/_static/notebooks/gentle-introduction-to-apache-spark.html","displayName":"Introduction to Apache Spark on Databricks","icon":"img/home/Python_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/databricks-for-data-scientists.html","displayName":"Databricks for Data Scientists","icon":"img/home/Scala_icon.svg"},{"linkURI":"https://docs.databricks.com/_static/notebooks/structured-streaming-python.html","displayName":"Introduction to Structured Streaming","icon":"img/home/Python_icon.svg"}],"enableClusterStart":true,"enableEBSVolumesUIByTier":false,"upgradeURL":"https://accounts.cloud.databricks.com/registration.html#login","notebookLoadingBackground":"#fff","sshContainerForwardedPort":2200,"enableServerAutoComplete":true,"enableStaticHtmlImport":true,"enableInstanceProfilesByTier":false,"defaultMemoryPerContainerMB":6000,"enablePresenceUI":true,"accounts":true,"useOnDemandClustersByDefault":true,"useFramedStaticNotebooks":false,"enableNewProgressReportUI":true,"defaultCoresPerContainer":4,"showTerminationReason":true,"enableNewClustersGet":true,"showPricePerDBU":false,"showSqlProxyUI":true};</script>
<script>var __DATABRICKS_NOTEBOOK_MODEL = {"version":"NotebookV1","origId":3827989904050330,"name":"Big_data_project_pum_it_up","language":"python","commands":[{"version":"CommandV1","origId":3827989904050334,"guid":"97167d91-9195-4e4b-a564-2b34bb98d38e","subtype":"command","commandType":"auto","position":3.0,"command":"%md\n#![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n# **Big data class - Final project**\n \n## **Predicting Pumps status - Pump It Up Data Driven competition**\n \nCan you predict which water pumps are faulty?\n\nUsing data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? This is an intermediate-level practice competition. Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.\nhttps://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/\n \nIn this Project:\n* *Part 1*: Load Data\n* *Part 1*: Pre-process data\n* *Part 2*: Train and tune models\n* *Part 3*: Predict model accuracy using SVM (sklearn)\n* *Part 4*: Predict model accuracy using Random Forest (MLlib)\n\nLook for below variable to control training data set size\n* DEBUG_SMALL = True (current value)- will train on 5K dataset, 20% of full training set is set for validation\n* DEBUG_SMALL = False - will train on full 80% of 50K dataset, 20% is set for validation\n ","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405143,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"03afc68b-8b68-49b1-a82d-dcb5b8ebece1"},{"version":"CommandV1","origId":3827989904050335,"guid":"b03130b9-a9a5-4f6c-838c-30f4fede6903","subtype":"command","commandType":"auto","position":4.0,"command":"projectVersion = '1.0.0'","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1493493405255,"submitTime":1493493405150,"finishTime":1493493405278,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"942c2010-d277-4d37-b470-4642bd025d7c"},{"version":"CommandV1","origId":3827989904050336,"guid":"01456e01-fcf3-47b6-93d9-fed3194cbe26","subtype":"command","commandType":"auto","position":5.0,"command":"%md\n#### Code and Libraries\n- Python and Spark\n- XGBoost\n- MLib\n","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405154,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"2c516612-fb78-493f-8cba-2854b019406e"},{"version":"CommandV1","origId":3827989904050337,"guid":"293dcdca-7de9-4828-bd2f-254f4b366706","subtype":"command","commandType":"auto","position":5.5,"command":"# Data files for this assignment can be found at:\n#Two locations:\n#  s9rayjpv1493320587137\n#  5hbwc80e1492906464748\n#/FileStore/tables/s9rayjpv1493320587137/Test_set_values.csv\n#/FileStore/tables/s9rayjpv1493320587137/Training_set_labels.csv\n#/FileStore/tables/s9rayjpv1493320587137/Training_data_values.csv\ndisplay(dbutils.fs.ls('/FileStore/tables/5hbwc80e1492906464748'))","commandVersion":0,"state":"finished","results":{"type":"table","data":[["dbfs:/FileStore/tables/5hbwc80e1492906464748/Test_set_values.csv","Test_set_values.csv",5016337],["dbfs:/FileStore/tables/5hbwc80e1492906464748/Training_data_values.csv","Training_data_values.csv",20069199],["dbfs:/FileStore/tables/5hbwc80e1492906464748/Training_set_labels.csv","Training_set_labels.csv",1148327]],"arguments":{},"addedWidgets":{},"removedWidgets":[],"schema":[{"name":"path","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"size","type":"\"long\"","metadata":"{}"}],"overflow":false,"aggData":[],"aggSchema":[],"aggOverflow":false,"aggSeriesLimitReached":false,"aggError":"","aggType":"","plotOptions":null,"isJsonSchema":true,"dbfsResultPath":null},"errorSummary":null,"error":null,"workflows":[],"startTime":1493493405282,"submitTime":1493493405177,"finishTime":1493493405886,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"1392.4","height":"197.4","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"647b10bf-a627-4903-be8b-a9473ffc7f8d"},{"version":"CommandV1","origId":3827989904050338,"guid":"764f411c-0783-44e1-8985-bcb6f53b89bf","subtype":"command","commandType":"auto","position":6.5,"command":"%md **WARNING:** If *problem with import*, required in the cell below, is not installed, follow the instructions [here](https://databricks-staging-cloudfront.staging.cloud.databricks.com/public/c65da9a2fa40e45a2028cddebe45b54c/8637560089690848/4187311313936645/6977722904629137/05f3c2ecc3.html).\n\n**Pre-processing functions**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405182,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7204e3e0-80f6-4e27-9b83-b2cdcce97fb1"},{"version":"CommandV1","origId":3377476042672112,"guid":"d32c3731-04c8-44c8-b839-3511edf81eec","subtype":"command","commandType":"auto","position":6.75,"command":"import sys\nimport os\nfrom test_helper import Test\nimport pandas as pd\nimport sklearn\nfrom sklearn import preprocessing\nfrom sklearn.cross_validation import train_test_split\n\nsklearn.__version__","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">38</span><span class=\"ansired\">]: </span>&apos;0.17.1&apos;\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1493493405890,"submitTime":1493493405191,"finishTime":1493493405960,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"4e657070-9725-463b-bd20-05f4c783282d"},{"version":"CommandV1","origId":3827989904050339,"guid":"6a483e10-898b-4194-9ae9-baafe160af56","subtype":"command","commandType":"auto","position":7.0,"command":"import sys\nimport os\nfrom test_helper import Test\nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import preprocessing\nimport random\n#from sklearn.model_selection import train_test_split\n\ndef load_data():\n    train_df = pd.read_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/Training_data_values.csv\")\n    train_label_df = pd.read_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/Training_set_labels.csv\")\n    test_df    = pd.read_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/Test_set_values.csv\")\n    \n    #train_df = train_df.drop([\"id\"],axis=1)\n    #test_df = test_df.drop([\"Id\"],axis=1)\n    return train_df, train_label_df, test_df\n\ndef drop_add_features(train_df,test_df,train_lbl_df):\n    '''\n    Remove columns:\n    1. recorded_by - same value for each instance\n    2. payment because same as payment_type \n    3. quantity_group same as quantity\n    4. waterpoint_type_group same as waterpoint_type\n    \n    Drop \"Id\" column from Train data and Train labels\n    \n    Add columns:\n    1. Add year, month and day based on date_recorded\n    '''\n    drop_columns=['recorded_by','payment','quantity_group','waterpoint_type_group']\n    date_columns = ['date_recorded']\n    \n    for df in [train_df, test_df]:\n        for col in date_columns:\n            df[col] = pd.to_datetime(df[col])\n            df[col + \"_day\"] = df[col].dt.dayofyear\n            df[col + \"_month\"] = df[col].dt.month\n            df[col + \"_year\"] = df[col].dt.year\n        df.drop(drop_columns+date_columns,axis=1,inplace=True)\n\n    for df in [train_df, train_lbl_df]:\n        df.drop('id',axis=1,inplace=True) \n     \ndef map_label_to_int(label):\n    if label=='functional':\n        return 0\n    elif label =='non functional':\n        return 1\n    else: # functional needs repair\n        return 2\n\ndef map_int_to_label(label):\n    if label == 0:\n        return 'functional'\n    elif label == 1:\n        return 'non functional'\n    else: # 2\n        return 'functional needs repair'\n\ndef map_float_to_label(label):\n    if label <= 0.5:\n        return 'functional'\n    elif label > 0.5 and label < 1.5:\n        return 'non functional'\n    else: # 2\n        return 'functional needs repair'\n    \ndef pre_process_data(train_df,test_df,train_lbl_df):\n    ''' \n        Encode all categorical columns (defined as 'object' in data-frame\n    '''     \n    \n    missing_columns=['public_meeting','permit','waterpoint_type']\n    for f in missing_columns:\n        train_df[f].fillna('Missing', inplace=True)\n        test_df[f].fillna('Missing', inplace=True)\n    \n    print(\"Before Categorical pre-processing - number of features\",len(train_df.columns))\n    categorical=['permit','public_meeting','source_class','quantity','management_group','quality_group','waterpoint_type','source_type','payment_type',\\\n                 'extraction_type_class','water_quality','basin','source']\n    train_df = pd.get_dummies(train_df,prefix_sep='_',columns=categorical)\n    test_df = pd.get_dummies(test_df,prefix_sep='_',columns=categorical)\n    print(\"After Categorical pre-processing - number of features\",len(train_df.columns))\n    \n    for f in train_df.columns:\n        if train_df[f].dtype == 'object':\n            #print(\"processing Column:..\",f)\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(np.unique(list(train_df[f].values) + list(test_df[f].values)))\n            train_df[f] = lbl.transform(list(train_df[f].values))\n            test_df[f]       = lbl.transform(list(test_df[f].values))\n    \n    # fill NaN values\n    for f in train_df.columns:\n        if train_df[f].dtype in ['float64','int64']:\n            train_df[f].fillna(train_df[f].mean(), inplace=True)\n            test_df[f].fillna(test_df[f].mean(), inplace=True)\n    \n    train_lbl_df['status_group'] = train_lbl_df['status_group'].apply(map_label_to_int)\n        \n    return train_df,test_df, train_lbl_df\ntrain_df, train_lbl_df, test_df = load_data()\ndrop_add_features(train_df,test_df,train_lbl_df)\ntrain_df,test_df,train_lbl_df = pre_process_data(train_df, test_df, train_lbl_df)","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">(&apos;Before Categorical pre-processing - number of features&apos;, 37)\n(&apos;After Categorical pre-processing - number of features&apos;, 104)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1493493405963,"submitTime":1493493405194,"finishTime":1493493409184,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"dc963ee7-9789-471a-9d25-1c65e5cace04"},{"version":"CommandV1","origId":3827989904050340,"guid":"71e7465e-cb39-4abc-ad4a-860a4a6e3154","subtype":"command","commandType":"auto","position":7.5,"command":"%md\n#### **SVM model**\nUse sklearn library SVM model to perform multi-class classification. For demo perposes the flag is set to run on small reaining set\n* DEBUG_SMALL = True - if set to True only 5K samples will be used for training\n* FINAL_RUN = False  - if set to False will not train model on full data set","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405199,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"46ebece8-5484-49b1-933a-60439a715bf6"},{"version":"CommandV1","origId":2952614701094783,"guid":"147e3990-8331-41b4-be71-5dec4ac9e8e2","subtype":"command","commandType":"auto","position":7.875,"command":"%md\n#### *Helper Functions*","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405208,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"08faaa76-6466-4832-97d6-0d22c0fb45ba"},{"version":"CommandV1","origId":3377476042672113,"guid":"790c64f5-4af8-4cd9-abb0-944fd1bd57b6","subtype":"command","commandType":"auto","position":8.25,"command":"from sklearn import metrics\nimport time\ndef train_svc(X_train, Y_train):\n    clf = svm.SVC(decision_function_shape='ovo')\n    clf.fit(X_train, Y_train['status_group']) \n#     SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n#         decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\n#         max_iter=-1, probability=False, random_state=None, shrinking=True,\n#         tol=0.001, verbose=False)\n    \n    return clf\n\ndef predict_validation_result(model,X_validate,Y_validate):\n    labels = model.predict(X_validate)\n    print(metrics.confusion_matrix(Y_validate,labels))\n\n    return metrics.accuracy_score(Y_validate,labels),labels\n\ndef scale_data(train_df, test_df):\n    '''\n    It is important to scale data for the SVM algorithm\n    Use sklearn scaler to scale validation/testing data based on training data\n    '''\n    \n    scaler = preprocessing.StandardScaler().fit(train_df)\n    train_df = scaler.transform(train_df)\n    test_df = scaler.transform(test_df)\n    \n    return train_df,test_df\n\ndef create_submission(model,test_df, test_df_ix):\n    '''\n    Run prediction using model and test_df and create submission file\n    \n    '''\n    predictions = model.predict(test_df)\n    predictions_label=[]\n    for pred in predictions:\n        predictions_label.append(map_int_to_label(pred))\n    \n    submission = pd.DataFrame(data=predictions_label,  # values\n                              index=test_df_ix,  # 1st column as index\n                              columns=[\"status_group\"])  # 1st row as the column names\n    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n    submission.to_csv(\"/dbfs/FileStore/tables/5hbwc80e1492906464748/submission_svm_\"+timestr+\".csv\")","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\"></div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1493493409187,"submitTime":1493493405218,"finishTime":1493493409258,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"0cc35b6e-3647-46ce-a7fc-2024d7f457dd"},{"version":"CommandV1","origId":2952614701094784,"guid":"d2e56a82-70e9-41b8-87a8-63db0ad488a5","subtype":"command","commandType":"auto","position":8.625,"command":"%md\n#### **SVM - main run**","commandVersion":0,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405222,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"23c821ab-cba1-4b16-8e8f-45ac691dccc5"},{"version":"CommandV1","origId":3827989904050341,"guid":"df95489b-229e-4ab1-91d1-215e15381c4f","subtype":"command","commandType":"auto","position":9.0,"command":"from sklearn import svm\n\nDEBUG_SMALL = True\nFINAL_RUN = False\ntrain_df, train_lbl_df, test_df = load_data()\nprint('Data is loaded')\ndrop_add_features(train_df,test_df,train_lbl_df)\ntrain_df,test_df,train_lbl_df = pre_process_data(train_df, test_df, train_lbl_df)\ntest_df_ix=test_df['id']\ntest_df.drop('id',axis=1,inplace=True)\ntrain_df,test_df = scale_data(train_df,test_df)\nrandom.seed(1234)\n\n'''\nX_train - data frame to be used for training\nY_train - label data corresponding to X_train\n'''\n\nX_train, X_validate, Y_train, Y_validate = train_test_split(train_df, train_lbl_df, test_size=0.20,random_state = 2015)\n\nif FINAL_RUN == False:\n    if DEBUG_SMALL:\n        print('Running training on small sample')\n        clf = train_svc(X_train[0:5000], Y_train[0:5000])\n    else:\n        print('Running training on full sample data')\n        clf = train_svc(X_train, Y_train)\n    accuracy, predictions = predict_validation_result(clf,X_validate,Y_validate)\n    print(\"Accuracy on validation set\", accuracy)\n    #create_submission(clf, test_df,test_df_ix )\nelse:\n    '''\n    In case submission, run model on full data set\n    '''\n    print('Running training on full data')\n    clf = train_svc(train_df, train_lbl_df)\n    #create_submission(clf, test_df,test_df_ix)","commandVersion":1,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Data is loaded\n(&apos;Before Categorical pre-processing - number of features&apos;, 37)\n(&apos;After Categorical pre-processing - number of features&apos;, 104)\nRunning training on small sample\n[[5923  515   22]\n [1809 2770   15]\n [ 660  118   48]]\n(&apos;Accuracy on validation set&apos;, 0.73577441077441075)\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":null,"error":null,"workflows":[],"startTime":1493493954039,"submitTime":1493493954026,"finishTime":1493493970538,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"7b01b0e0-9d1b-49db-8620-f50df5c0006d"},{"version":"CommandV1","origId":3827989904050342,"guid":"a803e8b8-8d25-4a92-a09f-9416cd43791a","subtype":"command","commandType":"auto","position":10.0,"command":"%md\n#### **Random Forest model**\nUse MLlib Spark library and pipeline to perform multi-class classification. For demo perposes the flag is set to run on small reaining set\n* DEBUG_SMALL = True - if set to True only 5K samples will be used for training\n* FINAL_RUN = False  - if set to False will not train model on full data set","commandVersion":1,"state":"finished","results":null,"errorSummary":null,"error":null,"workflows":[],"startTime":0,"submitTime":1493493405234,"finishTime":0,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"46038dc3-0754-4fd1-a786-9ffa0d9e654b"},{"version":"CommandV1","origId":154854574029696,"guid":"8f3011c1-dd42-41a7-a3c6-33bd25db3bb6","subtype":"command","commandType":"auto","position":11.0,"command":"from pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\nDEBUG_SMALL = True\nFINAL_RUN = False\ntrain_df, train_lbl_df, test_df = load_data()\nprint('Data is loaded')\ndrop_add_features(train_df,test_df,train_lbl_df)\ntrain_df,test_df,train_lbl_df = pre_process_data(train_df, test_df, train_lbl_df)\n#print(train_lbl_df)\ntrain_columns = list(train_df.columns)\n#print(train_columns)\ntrain_df['status_group'] = train_lbl_df['status_group']\n#print(train_df.head())\ntest_df_ix=test_df['id']\ntest_df.drop('id',axis=1,inplace=True)\n\n''' Create Spark Data Frame and add features/labels for the MLlib\n'''\nif DEBUG_SMALL:\n  print(\"Running training on small data-set\")\n  traindf = sqlContext.createDataFrame(train_df[0:5000])\nelse:\n  print(\"Running training on 80% data-set\")\n  traindf = sqlContext.createDataFrame(train_df)\n\n# Below transformations are done in order to brind data-frame to the format MLlib is requiring.\n# MLlib requires data-frame with two columns: labels and features. While features column is collection of all features\n#\nlabelIndexer = StringIndexer(inputCol=\"status_group\", outputCol=\"indexedLabel\").fit(traindf)\nassembler = VectorAssembler(inputCols=train_columns, outputCol=\"features\")\ntraindf = assembler.transform(traindf)\nfeatureIndexer =VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=3).fit(traindf)\n\n(trainingData, testData) = traindf.randomSplit([0.8, 0.2])\n\n\n#\n#Best params from sklearn {'n_estimators': 120, 'random_state': 1, 'min_samples_split': 5, 'max_features': 50, 'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1}\n#\n# MLlib RandomForestClassifier input (from documentation)\n# class pyspark.ml.classification.RandomForestClassifier(self, featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", probabilityCol=\"probability\", rawPredictionCol=\"rawPrediction\", maxDepth=5, \n#maxBins=32, minInstancesPerNode=1, minInfoGain=0.0, maxMemoryInMB=256, cacheNodeIds=False, checkpointInterval=10, impurity=\"gini\", numTrees=20, featureSubsetStrategy=\"auto\", seed=None, subsamplingRate=1.0)\n\n#########################################################################\n# Below code implementation is adopted from Spark MLlib main guide\n# https://spark.apache.org/docs/2.0.2/ml-classification-regression.html\n##########################################################################\n#Train a RandomForest model.\n#\nrf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxDepth=19,numTrees=100, seed=1)\n\n# Convert indexed labels back to original labels.\nlabelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n                               labels=labelIndexer.labels)\n\n# Chain indexers and forest in a Pipeline\npipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n\n# Train model.  This also runs the indexers.\nmodel = pipeline.fit(trainingData)\n\n# Make predictions.\npredictions = model.transform(testData)\n\n# Select example rows to display.\npredictions.select(\"predictedLabel\", \"status_group\", \"features\").show(5)\n\n# Select (prediction, true label) and compute test error\nevaluator = MulticlassClassificationEvaluator(\n    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\naccuracy = evaluator.evaluate(predictions)\nprint(\"Test Accuracy = %g\" % (accuracy))\n\nrfModel = model.stages[2]\nprint(rfModel)  # summary only\n","commandVersion":0,"state":"finished","results":{"type":"html","data":"<div class=\"ansiout\">Data is loaded\n(&apos;Before Categorical pre-processing - number of features&apos;, 37)\n(&apos;After Categorical pre-processing - number of features&apos;, 104)\nRunning training on small data-set\n+--------------+------------+--------------------+\n|predictedLabel|status_group|            features|\n+--------------+------------+--------------------+\n|             1|           1|(104,[1,2,3,4,5,6...|\n|             1|           1|(104,[1,2,3,4,5,6...|\n|             1|           1|(104,[1,2,3,4,5,6...|\n|             1|           1|(104,[1,2,3,4,5,6...|\n|             1|           2|(104,[1,2,3,4,5,6...|\n+--------------+------------+--------------------+\nonly showing top 5 rows\n\nTest Accuracy = 0.763366\nRandomForestClassificationModel (uid=rfc_107768cb31bf) with 100 trees\n</div>","arguments":{},"addedWidgets":{},"removedWidgets":[]},"errorSummary":"Cancelled","error":null,"workflows":[],"startTime":1493493490318,"submitTime":1493493490308,"finishTime":1493493533676,"collapsed":false,"bindings":{},"inputWidgets":{},"displayType":"table","width":"auto","height":"auto","xColumns":null,"yColumns":null,"pivotColumns":null,"pivotAggregation":null,"customPlotOptions":{},"commentThread":[],"commentsVisible":false,"parentHierarchy":[],"diffInserts":[],"diffDeletes":[],"globalVars":{},"latestUser":"a user","commandTitle":"","showCommandTitle":false,"hideCommandCode":false,"hideCommandResult":false,"iPythonMetadata":null,"streamStates":{},"nuid":"56424c72-44c6-4c4f-bf1d-3236187c95e4"}],"dashboards":[],"guid":"3ee89496-671d-458f-99d1-4a1ed508c0a7","globalVars":{},"iPythonMetadata":null,"inputWidgets":{}};</script>
<script
 src="https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js"
 onerror="window.mainJsLoadError = true;"></script>
</head>
<body>
  <script>
if (window.mainJsLoadError) {
  var u = 'https://databricks-prod-cloudfront.cloud.databricks.com/static/e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855/js/notebook-main.js';
  var b = document.getElementsByTagName('body')[0];
  var c = document.createElement('div');
  c.innerHTML = ('<h1>Network Error</h1>' +
    '<p><b>Please check your network connection and try again.</b></p>' +
    '<p>Could not load a required resource: ' + u + '</p>');
  c.style.margin = '30px';
  c.style.padding = '20px 50px';
  c.style.backgroundColor = '#f5f5f5';
  c.style.borderRadius = '5px';
  b.appendChild(c);
}
</script>
</body>
</html>
